# ML Fairness

Analyze fairness of ML algorith against some sensitive data. Also it tries to mitigate bias of ML models. 

# Business Problem

<br>• We created ML classification problem that predicts whether a person is stopped by police.
<br>• Machine learning models can sometimes amplify existing biases that are present in the data they are trained on, leading to unfair and discriminatory outcomes. 
This can be particularly problematic in the case of sensitive attributes such as gender and race, as it can result in discrimination against certain groups of people.
<br>• It is aimed to ensure that ML models are fair and unbiased.

# Dataset

<br>•North Carolina Policing Dataset
<br>• contains features like gender and race

# Tools used

<br>• pandas and numpy (data manipulation)
<br>• matplotlib and seaborn (data visualization)
<br>• sklearn (ml model train, cross validation, and data preprocessing)
<br>• xgb and knn (models for classification problem)
